{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238adde3",
   "metadata": {},
   "source": [
    "##### Qn1. Write a program to demonstrate the working of Dimensionality reduction using Principle component Analysis method on a dataset iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f51657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA on Iris Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941af62",
   "metadata": {},
   "source": [
    "##### Qn2. Write a program to demonstrate the working of the decision treee based ID3 algorithm by considering a datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree (ID3) on iris dataset\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdc278",
   "metadata": {},
   "source": [
    "##### Qn3. Write a Python program to implement Simple Linear Regression by considering a dataset. Plot the confusion matrix and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17441dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression on diabetes dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data[:, None, 2]  # Use one feature\n",
    "y = diabetes.target\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Actual')\n",
    "plt.plot(X, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621efbe5",
   "metadata": {},
   "source": [
    "##### Qn4. Build KNN Classification model for a given dataset. Vary the number of values as follows and compare the results: 1, 3, 5, 7, 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classification on iris dataset with different k values\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "for k in [1, 3, 5, 7, 11]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f'Accuracy for k={k}: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1fc76",
   "metadata": {},
   "source": [
    "##### Qn5. Consider a dataset, use RandomForest to predict the output class. Vary the number of trees as follows and compare the results: 20, 50, 100, 200, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd965c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest on iris dataset with different number of trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for n in [20, 50, 100, 200, 500]:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(f'Accuracy for n_estimators={n}: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7b0a4",
   "metadata": {},
   "source": [
    "##### Qn6. Implement Support Vector Machine for a dataset and compare the accuracy by applying the following kernel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77de5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with different kernels on iris dataset\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(f'Accuracy with kernel={kernel}: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87f7c2",
   "metadata": {},
   "source": [
    "##### Qn7. Write a python program to implement K-Means clustering Algorithm. Vary the number of k values as follows and compare results: 1, 3, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a61ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering on iris dataset with different k values\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for k in [1, 3, 5]:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(iris.data)\n",
    "    print(f'Inertia for k={k}: {kmeans.inertia_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926008c6",
   "metadata": {},
   "source": [
    "##### Qn8. Write a program to implemet the naive Bayesian Classifier for a sim;e training data set stored as a CSV file. Compute the accuracy of the classifier, considering few test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier on iris dataset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(f'Naive Bayes Accuracy: {accuracy_score(y_test, y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c33703",
   "metadata": {},
   "source": [
    "##### Qn9. Implement Dimensionality reduction using Filter method(Chi square, Correlation regression, Mutual Information) feature selection techniques on heart disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38960ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection on heart disease dataset (using sklearn's sample dataset for demonstration)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, f_classif\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Chi-square\n",
    "chi2_selector = SelectKBest(chi2, k=5).fit(X, y)\n",
    "print(\"Chi2 selected features:\", chi2_selector.get_support(indices=True))\n",
    "\n",
    "# Correlation regression (ANOVA F-value)\n",
    "f_selector = SelectKBest(f_classif, k=5).fit(X, y)\n",
    "print(\"Correlation regression selected features:\", f_selector.get_support(indices=True))\n",
    "\n",
    "# Mutual Information\n",
    "mi_selector = SelectKBest(mutual_info_classif, k=5).fit(X, y)\n",
    "print(\"Mutual Information selected features:\", mi_selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe8741",
   "metadata": {},
   "source": [
    "##### Qn10. Implement Ensemble Learning technique on heart disease dataset using following methods: (Max Voting, Averaging, Weighted Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af0e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning on heart disease dataset (using sklearn's sample dataset for demonstration)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Max Voting\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('svc', SVC(probability=True)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "], voting='hard')\n",
    "voting.fit(X_train, y_train)\n",
    "print(\"Max Voting Accuracy:\", accuracy_score(y_test, voting.predict(X_test)))\n",
    "\n",
    "# Averaging (Soft Voting)\n",
    "voting_soft = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('svc', SVC(probability=True)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "], voting='soft')\n",
    "voting_soft.fit(X_train, y_train)\n",
    "print(\"Averaging (Soft Voting) Accuracy:\", accuracy_score(y_test, voting_soft.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
