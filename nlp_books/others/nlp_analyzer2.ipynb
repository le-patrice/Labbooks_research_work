{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8251633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('chunkers/maxent_ne_chunker')\n",
    "except LookupError:\n",
    "    nltk.download('maxent_ne_chunker')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c98fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinguisticAnalyzer:\n",
    "    \"\"\"Main class for analyzing linguistic evolution across novels.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Book metadata\n",
    "        self.books_data = {\n",
    "            'Tristram Shandy': {'year': 1759, 'themes': ['psychological', 'fragmented', 'narrative']},\n",
    "            'The Scarlet Letter': {'year': 1850, 'themes': ['religious', 'moral', 'puritan']},\n",
    "            'Sister Carrie': {'year': 1900, 'themes': ['urban', 'industrial', 'naturalism']},\n",
    "            'The Martian Chronicles': {'year': 1950, 'themes': ['futuristic', 'colonial', 'technology']},\n",
    "            'White Teeth': {'year': 2000, 'themes': ['multicultural', 'identity', 'modern']},\n",
    "            'The Vanishing Half': {'year': 2020, 'themes': ['race', 'identity', 'contemporary']}\n",
    "        }\n",
    "        \n",
    "        # Target keywords for analysis\n",
    "        self.target_keywords = [\n",
    "            'name', 'race', 'self', 'color', 'double', 'sin', 'shame', 'virtue', 'repent',\n",
    "            'rocket', 'earth', 'mars', 'colonize', 'soil', 'vanish', 'gone', 'half', \n",
    "            'missing', 'woman', 'freedom'\n",
    "        ]\n",
    "        \n",
    "        self.analyzed_books = {}\n",
    "        self.tfidf_matrix = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Comprehensive text preprocessing.\"\"\"\n",
    "        # Clean text\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and short words\n",
    "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def extract_linguistic_features(self, text, book_title):\n",
    "        \"\"\"Extract comprehensive linguistic features from text.\"\"\"\n",
    "        features = {\n",
    "            'title': book_title,\n",
    "            'year': self.books_data[book_title]['year'],\n",
    "            'word_tokens': [],\n",
    "            'named_entities': [],\n",
    "            'pos_tags': [],\n",
    "            'detailed_pos': [],\n",
    "            'lemmas': [],\n",
    "            'stems': [],\n",
    "            'bigrams': [],\n",
    "            'trigrams': [],\n",
    "            'tense_analysis': {},\n",
    "            'morphological_table': []\n",
    "        }\n",
    "        \n",
    "        # Preprocess\n",
    "        tokens = self.preprocess_text(text)\n",
    "        features['word_tokens'] = tokens\n",
    "        \n",
    "        # POS tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        features['pos_tags'] = pos_tags\n",
    "        \n",
    "        # Detailed linguistic analysis with spaCy\n",
    "        if nlp:\n",
    "            doc = nlp(text[:1000000])  # Limit for memory efficiency\n",
    "            for token in doc:\n",
    "                if not token.is_stop and not token.is_punct and len(token.text) > 2:\n",
    "                    features['detailed_pos'].append({\n",
    "                        'text': token.text.lower(),\n",
    "                        'lemma': token.lemma_,\n",
    "                        'pos': token.pos_,\n",
    "                        'tag': token.tag_,\n",
    "                        'dep': token.dep_,\n",
    "                        'is_alpha': token.is_alpha\n",
    "                    })\n",
    "            \n",
    "            # Named entities\n",
    "            features['named_entities'] = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # Lemmatization and stemming\n",
    "        for word, pos in pos_tags:\n",
    "            lemma = self.lemmatizer.lemmatize(word.lower())\n",
    "            stem = self.stemmer.stem(word.lower())\n",
    "            features['lemmas'].append(lemma)\n",
    "            features['stems'].append(stem)\n",
    "            \n",
    "            # Create morphological table entry\n",
    "            features['morphological_table'].append({\n",
    "                'word': word.lower(),\n",
    "                'lemma': lemma,\n",
    "                'stem': stem,\n",
    "                'pos': pos,\n",
    "                'morphemes': self._analyze_morphemes(word)\n",
    "            })\n",
    "        \n",
    "        # N-grams\n",
    "        features['bigrams'] = list(ngrams(tokens, 2))\n",
    "        features['trigrams'] = list(ngrams(tokens, 3))\n",
    "        \n",
    "        # Tense analysis\n",
    "        features['tense_analysis'] = self._analyze_tense(pos_tags)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _analyze_morphemes(self, word):\n",
    "        \"\"\"Simple morpheme analysis.\"\"\"\n",
    "        morphemes = []\n",
    "        if word.endswith('ing'):\n",
    "            morphemes.append('progressive')\n",
    "        if word.endswith('ed'):\n",
    "            morphemes.append('past')\n",
    "        if word.endswith('s') and not word.endswith('ss'):\n",
    "            morphemes.append('plural/3rd_person')\n",
    "        return morphemes\n",
    "    \n",
    "    def _analyze_tense(self, pos_tags):\n",
    "        \"\"\"Analyze tense distribution.\"\"\"\n",
    "        tense_counts = defaultdict(int)\n",
    "        for word, pos in pos_tags:\n",
    "            if pos.startswith('VB'):\n",
    "                if pos == 'VBD':\n",
    "                    tense_counts['past'] += 1\n",
    "                elif pos == 'VBG':\n",
    "                    tense_counts['present_progressive'] += 1\n",
    "                elif pos == 'VBN':\n",
    "                    tense_counts['past_participle'] += 1\n",
    "                elif pos == 'VBP' or pos == 'VBZ':\n",
    "                    tense_counts['present'] += 1\n",
    "                else:\n",
    "                    tense_counts['other'] += 1\n",
    "        return dict(tense_counts)\n",
    "    \n",
    "    def sentiment_analysis(self, text, method='vader'):\n",
    "        \"\"\"Perform sentiment analysis using VADER or Naive Bayes.\"\"\"\n",
    "        if method == 'vader':\n",
    "            scores = self.vader.polarity_scores(text)\n",
    "            return {\n",
    "                'compound': scores['compound'],\n",
    "                'positive': scores['pos'],\n",
    "                'negative': scores['neg'],\n",
    "                'neutral': scores['neu']\n",
    "            }\n",
    "        # Note: For Naive Bayes, would need labeled training data\n",
    "        return None\n",
    "    \n",
    "    def calculate_keyword_frequency(self, features, keywords=None):\n",
    "        \"\"\"Calculate frequency of specific keywords.\"\"\"\n",
    "        if keywords is None:\n",
    "            keywords = self.target_keywords\n",
    "            \n",
    "        keyword_freq = {}\n",
    "        tokens = features['word_tokens']\n",
    "        lemmas = features['lemmas']\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            # Count in both tokens and lemmas\n",
    "            token_count = tokens.count(keyword)\n",
    "            lemma_count = lemmas.count(keyword)\n",
    "            total_count = max(token_count, lemma_count)  # Avoid double counting\n",
    "            \n",
    "            keyword_freq[keyword] = {\n",
    "                'frequency': total_count,\n",
    "                'relative_frequency': total_count / len(tokens) if tokens else 0,\n",
    "                'contexts': self._extract_contexts(tokens, keyword)\n",
    "            }\n",
    "        \n",
    "        return keyword_freq\n",
    "    \n",
    "    def _extract_contexts(self, tokens, keyword, window=5):\n",
    "        \"\"\"Extract contexts around keyword occurrences.\"\"\"\n",
    "        contexts = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == keyword:\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(tokens), i + window + 1)\n",
    "                context = ' '.join(tokens[start:end])\n",
    "                contexts.append(context)\n",
    "        return contexts[:5]  # Limit contexts for memory\n",
    "    \n",
    "    def calculate_tfidf(self, book_texts):\n",
    "        \"\"\"Calculate TF-IDF scores across all books.\"\"\"\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            lowercase=True,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        texts = [text for text in book_texts.values()]\n",
    "        self.tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        self.feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Create TF-IDF DataFrame\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            self.tfidf_matrix.toarray(),\n",
    "            index=list(book_texts.keys()),\n",
    "            columns=self.feature_names\n",
    "        )\n",
    "        \n",
    "        return tfidf_df\n",
    "    \n",
    "    def calculate_inverse_term_frequency(self, tfidf_df):\n",
    "        \"\"\"Calculate upward and downward ITF.\"\"\"\n",
    "        itf_results = {}\n",
    "        \n",
    "        for book in tfidf_df.index:\n",
    "            book_scores = tfidf_df.loc[book]\n",
    "            other_books_mean = tfidf_df.drop(book).mean()\n",
    "            \n",
    "            # Upward ITF: words more frequent in this book\n",
    "            upward_itf = book_scores - other_books_mean\n",
    "            \n",
    "            # Downward ITF: words less frequent in this book\n",
    "            downward_itf = other_books_mean - book_scores\n",
    "            \n",
    "            itf_results[book] = {\n",
    "                'upward': upward_itf.nlargest(20).to_dict(),\n",
    "                'downward': downward_itf.nlargest(20).to_dict()\n",
    "            }\n",
    "        \n",
    "        return itf_results\n",
    "    \n",
    "    def analyze_book(self, text, book_title):\n",
    "        \"\"\"Complete analysis of a single book.\"\"\"\n",
    "        print(f\"Analyzing {book_title}...\")\n",
    "        \n",
    "        # Extract linguistic features\n",
    "        features = self.extract_linguistic_features(text, book_title)\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sentiment = self.sentiment_analysis(text)\n",
    "        \n",
    "        # Keyword frequency analysis\n",
    "        keyword_freq = self.calculate_keyword_frequency(features)\n",
    "        \n",
    "        # Compile results\n",
    "        analysis = {\n",
    "            'features': features,\n",
    "            'sentiment': sentiment,\n",
    "            'keyword_frequencies': keyword_freq,\n",
    "            'year': self.books_data[book_title]['year'],\n",
    "            'themes': self.books_data[book_title]['themes']\n",
    "        }\n",
    "        \n",
    "        self.analyzed_books[book_title] = analysis\n",
    "        return analysis\n",
    "    \n",
    "    def create_evolution_table(self):\n",
    "        \"\"\"Create table showing keyword evolution across time.\"\"\"\n",
    "        evolution_data = []\n",
    "        \n",
    "        for keyword in self.target_keywords:\n",
    "            for book_title, analysis in self.analyzed_books.items():\n",
    "                freq_data = analysis['keyword_frequencies'].get(keyword, {})\n",
    "                \n",
    "                evolution_data.append({\n",
    "                    'keyword': keyword,\n",
    "                    'book': book_title,\n",
    "                    'year': analysis['year'],\n",
    "                    'frequency': freq_data.get('frequency', 0),\n",
    "                    'relative_frequency': freq_data.get('relative_frequency', 0),\n",
    "                    'contexts': len(freq_data.get('contexts', [])),\n",
    "                    'sentiment': analysis['sentiment']['compound'] if analysis['sentiment'] else 0\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(evolution_data)\n",
    "    \n",
    "    def visualize_keyword_evolution(self, keyword, save_fig=False):\n",
    "        \"\"\"Create visualizations for keyword evolution.\"\"\"\n",
    "        evolution_df = self.create_evolution_table()\n",
    "        keyword_data = evolution_df[evolution_df['keyword'] == keyword].sort_values('year')\n",
    "        \n",
    "        if keyword_data.empty:\n",
    "            print(f\"No data found for keyword: {keyword}\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Line graph\n",
    "        ax1.plot(keyword_data['year'], keyword_data['relative_frequency'], \n",
    "                marker='o', linewidth=2, markersize=8)\n",
    "        ax1.set_title(f'Evolution of \"{keyword}\" - Relative Frequency Over Time')\n",
    "        ax1.set_xlabel('Year')\n",
    "        ax1.set_ylabel('Relative Frequency')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Bar graph\n",
    "        ax2.bar(keyword_data['year'], keyword_data['frequency'], \n",
    "               color='skyblue', alpha=0.7)\n",
    "        ax2.set_title(f'Evolution of \"{keyword}\" - Absolute Frequency')\n",
    "        ax2.set_xlabel('Year')\n",
    "        ax2.set_ylabel('Absolute Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_fig:\n",
    "            plt.savefig(f'{keyword}_evolution.png', dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Print semantic summary\n",
    "        self._print_keyword_summary(keyword, keyword_data)\n",
    "    \n",
    "    def _print_keyword_summary(self, keyword, keyword_data):\n",
    "        \"\"\"Print semantic and syntactic summary for keyword.\"\"\"\n",
    "        print(f\"\\n=== SUMMARY FOR '{keyword.upper()}' ===\")\n",
    "        print(f\"Time span: {keyword_data['year'].min()} - {keyword_data['year'].max()}\")\n",
    "        print(f\"Peak usage: {keyword_data.loc[keyword_data['frequency'].idxmax(), 'book']} \"\n",
    "              f\"({keyword_data['frequency'].max()} occurrences)\")\n",
    "        print(f\"Average relative frequency: {keyword_data['relative_frequency'].mean():.6f}\")\n",
    "        \n",
    "        # Context analysis\n",
    "        for _, row in keyword_data.iterrows():\n",
    "            if row['frequency'] > 0:\n",
    "                analysis = self.analyzed_books[row['book']]\n",
    "                contexts = analysis['keyword_frequencies'][keyword]['contexts']\n",
    "                if contexts:\n",
    "                    print(f\"\\n{row['book']} ({row['year']}) - Sample context:\")\n",
    "                    print(f\"  '{contexts[0]}'\")\n",
    "    \n",
    "    def interactive_word_analyzer(self):\n",
    "        \"\"\"Interactive function for user to analyze any word.\"\"\"\n",
    "        def analyze_word(word):\n",
    "            word = word.lower().strip()\n",
    "            print(f\"\\nAnalyzing word: '{word}'\")\n",
    "            \n",
    "            # Check if word exists in any book\n",
    "            word_data = []\n",
    "            for book_title, analysis in self.analyzed_books.items():\n",
    "                tokens = analysis['features']['word_tokens']\n",
    "                lemmas = analysis['features']['lemmas']\n",
    "                \n",
    "                token_count = tokens.count(word)\n",
    "                lemma_count = lemmas.count(word)\n",
    "                total_count = max(token_count, lemma_count)\n",
    "                \n",
    "                if total_count > 0:\n",
    "                    word_data.append({\n",
    "                        'book': book_title,\n",
    "                        'year': analysis['year'],\n",
    "                        'frequency': total_count,\n",
    "                        'relative_frequency': total_count / len(tokens) if tokens else 0\n",
    "                    })\n",
    "            \n",
    "            if not word_data:\n",
    "                print(f\"Word '{word}' not found in any of the analyzed books.\")\n",
    "                return\n",
    "            \n",
    "            # Create visualization\n",
    "            word_df = pd.DataFrame(word_data).sort_values('year')\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Line graph\n",
    "            ax1.plot(word_df['year'], word_df['relative_frequency'], \n",
    "                    marker='o', linewidth=2, markersize=8, color='red')\n",
    "            ax1.set_title(f'Evolution of \"{word}\" - Relative Frequency')\n",
    "            ax1.set_xlabel('Year')\n",
    "            ax1.set_ylabel('Relative Frequency')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Bar graph\n",
    "            ax2.bar(word_df['year'], word_df['frequency'], \n",
    "                   color='lightcoral', alpha=0.7)\n",
    "            ax2.set_title(f'Evolution of \"{word}\" - Absolute Frequency')\n",
    "            ax2.set_xlabel('Year')\n",
    "            ax2.set_ylabel('Absolute Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\n=== ANALYSIS SUMMARY FOR '{word.upper()}' ===\")\n",
    "            print(f\"Found in {len(word_data)} books\")\n",
    "            print(f\"Time span: {word_df['year'].min()} - {word_df['year'].max()}\")\n",
    "            print(f\"Peak usage: {word_df.loc[word_df['frequency'].idxmax(), 'book']} \"\n",
    "                  f\"({word_df['frequency'].max()} occurrences)\")\n",
    "            \n",
    "            return word_df\n",
    "        \n",
    "        return analyze_word\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate a comprehensive analysis report.\"\"\"\n",
    "        print(\"=== COMPREHENSIVE LINGUISTIC EVOLUTION REPORT ===\\n\")\n",
    "        \n",
    "        # Time-ordered analysis\n",
    "        sorted_books = sorted(self.analyzed_books.items(), \n",
    "                            key=lambda x: x[1]['year'])\n",
    "        \n",
    "        print(\"1. CHRONOLOGICAL OVERVIEW:\")\n",
    "        for book_title, analysis in sorted_books:\n",
    "            print(f\"\\n{book_title} ({analysis['year']}):\")\n",
    "            print(f\"  Themes: {', '.join(analysis['themes'])}\")\n",
    "            print(f\"  Sentiment: {analysis['sentiment']['compound']:.3f}\")\n",
    "            print(f\"  Total tokens: {len(analysis['features']['word_tokens'])}\")\n",
    "        \n",
    "        print(\"\\n2. KEYWORD EVOLUTION TRENDS:\")\n",
    "        evolution_df = self.create_evolution_table()\n",
    "        \n",
    "        # Find most evolving keywords\n",
    "        keyword_variance = evolution_df.groupby('keyword')['relative_frequency'].var().sort_values(ascending=False)\n",
    "        print(\"\\nMost variable keywords across time:\")\n",
    "        for keyword, variance in keyword_variance.head(10).items():\n",
    "            print(f\"  {keyword}: {variance:.8f}\")\n",
    "        \n",
    "        print(\"\\n3. CULTURAL SHIFT INDICATORS:\")\n",
    "        \n",
    "        # Technology vs Nature\n",
    "        tech_words = ['rocket', 'mars', 'colonize']\n",
    "        nature_words = ['earth', 'soil']\n",
    "        \n",
    "        tech_evolution = evolution_df[evolution_df['keyword'].isin(tech_words)].groupby('year')['relative_frequency'].sum()\n",
    "        nature_evolution = evolution_df[evolution_df['keyword'].isin(nature_words)].groupby('year')['relative_frequency'].sum()\n",
    "        \n",
    "        print(\"\\nTechnology vs Nature theme evolution:\")\n",
    "        for year in sorted(tech_evolution.index):\n",
    "            tech_freq = tech_evolution.get(year, 0)\n",
    "            nature_freq = nature_evolution.get(year, 0)\n",
    "            print(f\"  {year}: Tech={tech_freq:.6f}, Nature={nature_freq:.6f}\")\n",
    "        \n",
    "        # Identity and race\n",
    "        identity_words = ['name', 'race', 'self', 'color', 'double']\n",
    "        identity_evolution = evolution_df[evolution_df['keyword'].isin(identity_words)].groupby('year')['relative_frequency'].sum()\n",
    "        \n",
    "        print(\"\\nIdentity theme evolution:\")\n",
    "        for year in sorted(identity_evolution.index):\n",
    "            print(f\"  {year}: {identity_evolution.get(year, 0):.6f}\")\n",
    "        \n",
    "        return evolution_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800b6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and demonstration\n",
    "def main():\n",
    "    \"\"\"Main function demonstrating the analysis system.\"\"\"\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = LinguisticAnalyzer()\n",
    "    \n",
    "    print(\"Literary Language Evolution Analysis System\")\n",
    "    print(\"=========================================\")\n",
    "    print(\"\\nThis system analyzes linguistic evolution across six novels:\")\n",
    "    for book, data in analyzer.books_data.items():\n",
    "        print(f\"- {book} ({data['year']})\")\n",
    "    \n",
    "    print(f\"\\nTarget keywords: {', '.join(analyzer.target_keywords)}\")\n",
    "    \n",
    "    # Note: In practice, you would load actual book texts here\n",
    "    print(\"\\n[DEMO MODE - In practice, load actual book texts using:]\")\n",
    "    print(\"with open('book.txt', 'r', encoding='utf-8') as f:\")\n",
    "    print(\"    text = f.read()\")\n",
    "    print(\"    analyzer.analyze_book(text, 'Book Title')\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    sample_texts = {\n",
    "        'Tristram Shandy': \"The name of the self is double in nature, as is the color of shame...\",\n",
    "        'The Scarlet Letter': \"Sin and virtue repent in equal measure, the name bearing shame...\",\n",
    "        'Sister Carrie': \"The woman sought freedom in the city, her name gone from home...\",\n",
    "        'The Martian Chronicles': \"The rocket to Mars would colonize the red soil of Earth's neighbor...\",\n",
    "        'White Teeth': \"Identity and race color the modern self, names carrying double meaning...\",\n",
    "        'The Vanishing Half': \"Half the woman would vanish, her race and color gone missing...\"\n",
    "    }\n",
    "    \n",
    "    # Analyze sample texts\n",
    "    for title, text in sample_texts.items():\n",
    "        analyzer.analyze_book(text, title)\n",
    "    \n",
    "    # Generate evolution table and TF-IDF\n",
    "    evolution_df = analyzer.create_evolution_table()\n",
    "    tfidf_df = analyzer.calculate_tfidf(sample_texts)\n",
    "    itf_results = analyzer.calculate_inverse_term_frequency(tfidf_df)\n",
    "    \n",
    "    print(\"\\n=== SAMPLE ANALYSIS RESULTS ===\")\n",
    "    print(\"\\nEvolution table (first 10 rows):\")\n",
    "    print(evolution_df.head(10))\n",
    "    \n",
    "    print(\"\\n=== INTERACTIVE FEATURES ===\")\n",
    "    print(\"1. Use analyzer.visualize_keyword_evolution('keyword') for any target keyword\")\n",
    "    print(\"2. Use word_analyzer = analyzer.interactive_word_analyzer()\")\n",
    "    print(\"   Then: word_analyzer('your_word') to analyze any word\")\n",
    "    print(\"3. Use analyzer.generate_comprehensive_report() for full analysis\")\n",
    "    \n",
    "    # Demonstrate interactive analyzer\n",
    "    word_analyzer = analyzer.interactive_word_analyzer()\n",
    "    \n",
    "    print(\"\\n=== SYSTEM READY ===\")\n",
    "    print(\"The system is now ready for full-scale analysis with actual book texts.\")\n",
    "    \n",
    "    return analyzer, word_analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4b6a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Literary Language Evolution Analysis System\n",
      "=========================================\n",
      "\n",
      "This system analyzes linguistic evolution across six novels:\n",
      "- Tristram Shandy (1759)\n",
      "- The Scarlet Letter (1850)\n",
      "- Sister Carrie (1900)\n",
      "- The Martian Chronicles (1950)\n",
      "- White Teeth (2000)\n",
      "- The Vanishing Half (2020)\n",
      "\n",
      "Target keywords: name, race, self, color, double, sin, shame, virtue, repent, rocket, earth, mars, colonize, soil, vanish, gone, half, missing, woman, freedom\n",
      "\n",
      "[DEMO MODE - In practice, load actual book texts using:]\n",
      "with open('book.txt', 'r', encoding='utf-8') as f:\n",
      "    text = f.read()\n",
      "    analyzer.analyze_book(text, 'Book Title')\n",
      "Analyzing Tristram Shandy...\n",
      "Analyzing The Scarlet Letter...\n",
      "Analyzing Sister Carrie...\n",
      "Analyzing The Martian Chronicles...\n",
      "Analyzing White Teeth...\n",
      "Analyzing The Vanishing Half...\n",
      "\n",
      "=== SAMPLE ANALYSIS RESULTS ===\n",
      "\n",
      "Evolution table (first 10 rows):\n",
      "  keyword                    book  year  frequency  relative_frequency  \\\n",
      "0    name         Tristram Shandy  1759          1            0.166667   \n",
      "1    name      The Scarlet Letter  1850          1            0.125000   \n",
      "2    name           Sister Carrie  1900          1            0.142857   \n",
      "3    name  The Martian Chronicles  1950          0            0.000000   \n",
      "4    name             White Teeth  2000          1            0.111111   \n",
      "5    name      The Vanishing Half  2020          0            0.000000   \n",
      "6    race         Tristram Shandy  1759          0            0.000000   \n",
      "7    race      The Scarlet Letter  1850          0            0.000000   \n",
      "8    race           Sister Carrie  1900          0            0.000000   \n",
      "9    race  The Martian Chronicles  1950          0            0.000000   \n",
      "\n",
      "   contexts  sentiment  \n",
      "0         1    -0.4767  \n",
      "1         1    -0.5994  \n",
      "2         1     0.6369  \n",
      "3         0     0.0000  \n",
      "4         0     0.0000  \n",
      "5         0    -0.2960  \n",
      "6         0    -0.4767  \n",
      "7         0    -0.5994  \n",
      "8         0     0.6369  \n",
      "9         0     0.0000  \n",
      "\n",
      "=== INTERACTIVE FEATURES ===\n",
      "1. Use analyzer.visualize_keyword_evolution('keyword') for any target keyword\n",
      "2. Use word_analyzer = analyzer.interactive_word_analyzer()\n",
      "   Then: word_analyzer('your_word') to analyze any word\n",
      "3. Use analyzer.generate_comprehensive_report() for full analysis\n",
      "\n",
      "=== SYSTEM READY ===\n",
      "The system is now ready for full-scale analysis with actual book texts.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    analyzer, word_analyzer = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
