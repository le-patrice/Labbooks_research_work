import re
import warnings
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import textstat
from collections import Counter
import numpy as np
import nltk


# Set plot style and palette
plt.style.use('default')
sns.set_palette("husl")

# File paths from the user's uploaded files
FILE_PATHS = {
    "1853_Bartleby": "books/bartleby_1853.txt",
    "1892_Yellow": "books/yellow_1892.txt",
    "1929_Passing": "books/nella_1929.txt",
    "1988_Small": "books/small_place_1988.txt",
    "2014_Fem": "books/feminist_2014.txt"
}

def load_text_from_file(filepath):
    """Reads text content from a given file path."""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    # Remove source tags for cleaner analysis
    content = re.sub(r'\', '', content)
    return content

# Load full texts using the file paths
TEXTS = {label: load_text_from_file(filepath) for label, filepath in FILE_PATHS.items()}

SEMANTIC_FIELDS = {
    'Authority': ['power', 'control', 'authority', 'dominance', 'rule', 'command', 'minister', 'prime', 'government', 'state', 'empire', 'official'],
    'Identity': ['identity', 'self', 'race', 'racial', 'gender', 'woman', 'man', 'black', 'white', 'negro', 'nigger', 'feminist', 'human'],
    'Emotion': ['happy', 'sad', 'angry', 'joy', 'fear', 'love', 'hate', 'romantic', 'felicity', 'rage', 'bitterness', 'suffering', 'anxiety', 'melancholy', 'displeasure', 'irritation'],
    'Social': ['people', 'social', 'society', 'community', 'public', 'ordinary', 'men', 'women', 'family', 'friends', 'stranger', 'culture'],
    'Confinement': ['confined', 'prison', 'trap', 'cage', 'bars', 'restricted', 'limit', 'shut', 'locked', 'untamed', 'untenanted']
}

STOP = set(stopwords.words('english'))
VADER = SentimentIntensityAnalyzer()

def analyze_comprehensive(text, label):
    """Deep linguistic and semantic analysis"""
    tokens = [w.lower() for w in word_tokenize(text) if w.isalpha() and w.lower() not in STOP]
    year = int(label.split('_')[0])
    sents = sent_tokenize(text)
    
    # Handle empty text/tokens gracefully
    if not tokens:
        return {
            'label': label, 'year': year, 'word_count': 0, 'unique_words': 0, 'sentences': 0,
            'lexical_diversity': 0, 'avg_word_length': 0, 'avg_sent_length': 0,
            'flesch_score': 0, 'complexity_index': 0,
            'noun_ratio': 0, 'verb_ratio': 0, 'adj_ratio': 0, 'adv_ratio': 0,
            **{f'{field.lower()}_frequency': 0 for field in SEMANTIC_FIELDS},
            **{f'{field.lower()}_density': 0 for field in SEMANTIC_FIELDS},
            'vader_positive': 0, 'vader_negative': 0, 'vader_compound': 0, # Removed polarity and subjectivity
            'first_person_ratio': 0, 'second_person_ratio': 0, 'third_person_ratio': 0
        }

    pos_tags = pos_tag(tokens)
    
    # Core metrics
    record = {
        'label': label, 'year': year, 'word_count': len(tokens),
        'unique_words': len(set(tokens)), 'sentences': len(sents)
    }
    
    # Advanced linguistic features
    record.update({
        'lexical_diversity': len(set(tokens)) / len(tokens),
        'avg_word_length': np.mean([len(w) for w in tokens]),
        'avg_sent_length': len(tokens) / len(sents) if sents else 0,
        'flesch_score': textstat.flesch_reading_ease(text),
        'complexity_index': textstat.flesch_kincaid_grade(text)
    })
    
    # POS tag analysis - critical for understanding linguistic evolution
    pos_counts = Counter(tag for _, tag in pos_tags)
    total_pos = sum(pos_counts.values())
    
    grouped_pos_counts = {
        'NN': sum(v for k, v in pos_counts.items() if k.startswith('NN')),
        'VB': sum(v for k, v in pos_counts.items() if k.startswith('VB')),
        'JJ': sum(v for k, v in pos_counts.items() if k.startswith('JJ')),
        'RB': sum(v for k, v in pos_counts.items() if k.startswith('RB'))
    }

    record.update({
        'noun_ratio': grouped_pos_counts['NN'] / total_pos if total_pos else 0,
        'verb_ratio': grouped_pos_counts['VB'] / total_pos if total_pos else 0,
        'adj_ratio': grouped_pos_counts['JJ'] / total_pos if total_pos else 0,
        'adv_ratio': grouped_pos_counts['RB'] / total_pos if total_pos else 0
    })
    
    # Semantic field analysis - tracks thematic evolution
    for field, words in SEMANTIC_FIELDS.items():
        count = sum(1 for token in tokens if token in words)
        record[f'{field.lower()}_frequency'] = count
        record[f'{field.lower()}_density'] = (count / len(tokens) * 1000) if tokens else 0
    
    # Sentiment evolution - emotional trajectory analysis (using VADER only)
    vader_scores = VADER.polarity_scores(text)
    record.update({
        'vader_positive': vader_scores['pos'],
        'vader_negative': vader_scores['neg'],
        'vader_compound': vader_scores['compound']
    })
    
    # Pronoun analysis - perspective and voice evolution
    pronouns = {'first': ['i', 'me', 'my', 'we', 'us', 'our'], 
                'second': ['you', 'your', 'yours'],
                'third': ['he', 'him', 'his', 'she', 'her', 'they', 'them', 'it', 'its']}
    
    for perspective, pron_list in pronouns.items():
        count = sum(1 for token in tokens if token in pron_list)
        record[f'{perspective}_person_ratio'] = count / len(tokens) if tokens else 0
    
    return record

def create_visualizations(df):
    """Generate comprehensive bar and line visualizations"""
    
    # 1. Line Graphs for Trends Over Time
    fig1, axes1 = plt.subplots(2, 2, figsize=(16, 12))
    axes1 = axes1.flatten()

    # Semantic fields over time
    semantic_fields_to_plot = ['authority', 'identity', 'emotion', 'social'] # Exclude 'confinement' for this general trend if needed, or adjust subplot layout
    for i, field in enumerate(semantic_fields_to_plot):
        if i < len(axes1):
            density_col = f'{field}_density'
            if density_col in df.columns:
                axes1[i].plot(df['year'], df[density_col], 'o-', label=field.title(), linewidth=2, markersize=8)
                axes1[i].set_title(f'{field.title()} Density Over Time', fontweight='bold', fontsize=12)
                axes1[i].set_xlabel('Year')
                axes1[i].set_ylabel('Density (per 1000 words)')
                axes1[i].grid(True, alpha=0.3)
                axes1[i].legend()

    plt.tight_layout()
    plt.savefig('semantic_field_trends.png')
    
    fig2, axes2 = plt.subplots(2, 2, figsize=(16, 12))
    axes2 = axes2.flatten()

    # POS tag evolution
    pos_metrics = ['noun_ratio', 'verb_ratio', 'adj_ratio', 'adv_ratio']
    colors = ['blue', 'red', 'green', 'orange']
    for i, metric in enumerate(pos_metrics):
        axes2[i].plot(df['year'], df[metric], 'o-', label=metric.replace('_', ' ').title(), 
                     color=colors[i], linewidth=2, markersize=6)
        axes2[i].set_title(f'{metric.replace("_", " ").title()} Evolution', fontweight='bold', fontsize=12)
        axes2[i].set_xlabel('Year')
        axes2[i].set_ylabel('Ratio')
        axes2[i].grid(True, alpha=0.3)
        axes2[i].legend()

    plt.tight_layout()
    plt.savefig('pos_tag_trends.png')

    fig3, axes3 = plt.subplots(1, 2, figsize=(16, 6))

    # Complexity and readability trends
    axes3[0].plot(df['year'], df['flesch_score'], 'o-', color='purple', linewidth=2, markersize=8, label='Flesch Score (Higher = Easier)')
    axes3_twin = axes3[0].twinx()
    axes3_twin.plot(df['year'], df['avg_word_length'], 's--', color='brown', linewidth=2, markersize=6, label='Avg Word Length')
    axes3[0].set_title('Linguistic Complexity Evolution', fontweight='bold', fontsize=12)
    axes3[0].set_xlabel('Year')
    axes3[0].set_ylabel('Flesch Reading Ease', color='purple')
    axes3_twin.set_ylabel('Average Word Length', color='brown')
    axes3[0].grid(True, alpha=0.3)
    axes3[0].legend(loc='upper left')
    axes3_twin.legend(loc='upper right')
    
    # Sentiment trajectory (VADER only)
    axes3[1].plot(df['year'], df['vader_compound'], 's-', color='darkred', linewidth=2, markersize=8, label='VADER Compound Sentiment')
    axes3[1].set_title('Emotional Sentiment Evolution (VADER)', fontweight='bold', fontsize=12)
    axes3[1].set_xlabel('Year')
    axes3[1].set_ylabel('Compound Sentiment Score')
    axes3[1].legend()
    axes3[1].grid(True, alpha=0.3)
    axes3[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('complexity_sentiment_trends.png')

    # 2. Bar Charts for Comparative Analysis
    fig4, axes4 = plt.subplots(2, 2, figsize=(16, 12))
    axes4 = axes4.flatten()

    # Word usage frequency by era
    labels_short = [label.split('_')[1] for label in df['label']]
    x_pos = np.arange(len(df))
    width = 0.2
    
    axes4[0].bar(x_pos - width, df['word_count'], width, label='Word Count')
    axes4[0].bar(x_pos, df['unique_words'], width, label='Unique Words')
    axes4[0].bar(x_pos + width, df['lexical_diversity']*100, width, label='Lexical Diversity (x100)')
    axes4[0].set_title('Word Usage Patterns Across Eras', fontweight='bold')
    axes4[0].set_xlabel('Literary Works')
    axes4[0].set_xticks(x_pos)
    axes4[0].set_xticklabels(labels_short, rotation=45, ha='right')
    axes4[0].legend()
    
    # Semantic field comparison (stacked bar chart)
    semantic_cols_freq = [f'{field.lower()}_frequency' for field in SEMANTIC_FIELDS.keys()]
    semantic_df = df[semantic_cols_freq]
    semantic_df.columns = [col.replace('_frequency', '').title() for col in semantic_df.columns]
    semantic_df.index = labels_short
    semantic_df.plot(kind='bar', stacked=True, ax=axes4[1], width=0.8)
    axes4[1].set_title('Semantic Field Distribution', fontweight='bold')
    axes4[1].set_xlabel('Literary Works')
    axes4[1].set_ylabel('Frequency Count')
    axes4[1].tick_params(axis='x', rotation=45)
    axes4[1].legend(title="Semantic Fields", bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # Pronoun perspective analysis
    pronoun_data = df[['first_person_ratio', 'second_person_ratio', 'third_person_ratio']]
    pronoun_data.index = labels_short
    pronoun_data.plot(kind='bar', ax=axes4[2], width=0.8)
    axes4[2].set_title('Narrative Perspective Evolution (Pronoun Ratios)', fontweight='bold')
    axes4[2].set_xlabel('Literary Works')
    axes4[2].set_ylabel('Ratio')
    axes4[2].tick_params(axis='x', rotation=45)
    axes4[2].legend(['First Person', 'Second Person', 'Third Person'])
    
    # Readability comparison
    readability_df = df[['flesch_score', 'complexity_index', 'avg_sent_length']]
    readability_df.index = labels_short
    readability_df.plot(kind='bar', ax=axes4[3], width=0.8)
    axes4[3].set_title('Linguistic Readability Comparison', fontweight='bold')
    axes4[3].set_xlabel('Literary Works')
    axes4[3].set_ylabel('Score/Length')
    axes4[3].tick_params(axis='x', rotation=45)
    axes4[3].legend(['Flesch Reading Ease', 'Flesch-Kincaid Grade Level', 'Avg Sentence Length'])
    
    plt.tight_layout()
    plt.savefig('comparative_analysis.png')

    plt.close('all')

def generate_insights(df):
    """Deep analytical insights from the data"""
    print("\n🔍 COMPREHENSIVE LITERARY EVOLUTION ANALYSIS")
    print("=" * 60)
    
    print(f"\n📊 CORPUS OVERVIEW:")
    print(f"Total texts analyzed: {len(df)}")
    print(f"Time span: {df['year'].min()}-{df['year'].max()} ({df['year'].max()-df['year'].min()} years)")
    print(f"Average words per text: {df['word_count'].mean():.1f}")
    
    print(f"\n📈 KEY EVOLUTIONARY TRENDS:")
    
    # Lexical evolution
    diversity_trend = np.corrcoef(df['year'], df['lexical_diversity'])[0,1]
    print(f"Lexical Diversity Evolution: {diversity_trend:.3f} {'↗️ Increasing' if diversity_trend > 0 else '↘️ Decreasing'} (Correlation with Year)")
    
    # Semantic evolution
    print(f"\n🎭 SEMANTIC FIELD EVOLUTION (Correlation with Year):")
    for field in SEMANTIC_FIELDS.keys():
        density_col = f'{field.lower()}_density'
        if density_col in df.columns:
            trend = np.corrcoef(df['year'], df[density_col])[0,1]
            direction = '↗️ Rising' if trend > 0 else '↘️ Declining'
            print(f"- {field.title()} terms: {trend:.3f} {direction}")
    
    # Linguistic complexity
    flesch_trend = np.corrcoef(df['year'], df['flesch_score'])[0,1]
    complexity_trend = np.corrcoef(df['year'], df['complexity_index'])[0,1]
    print(f"\n📚 READABILITY/COMPLEXITY EVOLUTION:")
    print(f"- Flesch Reading Ease (Higher = Easier): {flesch_trend:.3f} {'↗️ Increasing Readability' if flesch_trend > 0 else '↘️ Decreasing Readability'}")
    print(f"- Flesch-Kincaid Grade Level (Higher = More Complex): {complexity_trend:.3f} {'↗️ Increasing Complexity' if complexity_trend > 0 else '↘️ Decreasing Complexity'}")

    # Emotional trajectory (VADER only)
    vader_compound_trend = np.corrcoef(df['year'], df['vader_compound'])[0,1]
    print(f"\n💭 EMOTIONAL EVOLUTION (Correlation with Year):")
    print(f"- VADER Compound Sentiment: {vader_compound_trend:.3f} {'↗️ More Positive' if vader_compound_trend > 0 else '↘️ More Negative'}")
    
    # Most significant changes in POS ratios
    print(f"\n🎯 MOST DRAMATIC CHANGES IN GRAMMATICAL STRUCTURE (Change from earliest to latest text):")
    pos_changes = {}
    for col in ['noun_ratio', 'verb_ratio', 'adj_ratio', 'adv_ratio']:
        start_val = df[col].iloc[0]
        end_val = df[col].iloc[-1]
        pos_changes[col] = abs(end_val - start_val)
        direction = 'increase' if end_val > start_val else 'decrease'
        print(f"- {col.replace('_', ' ').title()}: {pos_changes[col]:.3f} ({direction})")
    
    # Narrative perspective changes
    print(f"\n🗣️ NARRATIVE PERSPECTIVE EVOLUTION (Correlation with Year):")
    for pron_type in ['first', 'second', 'third']:
        ratio_col = f'{pron_type}_person_ratio'
        trend = np.corrcoef(df['year'], df[ratio_col])[0,1]
        direction = '↗️ Increasing' if trend > 0 else '↘️ Decreasing'
        print(f"- {pron_type.title()} Person Pronoun Ratio: {trend:.3f} {direction}")

    return df

# Main execution
def main():
    # Analyze all texts
    data = [analyze_comprehensive(text, label) for label, text in TEXTS.items()]
    df = pd.DataFrame(data).sort_values('year')
    
    # Generate insights
    results_df = generate_insights(df)
    
    # Create visualizations
    create_visualizations(df)
    
    print(f"\n✅ ANALYSIS COMPLETE - {len(df)} texts analyzed across {df['year'].max()-df['year'].min()} years")
    print("\nKey findings reveal significant evolution in semantic focus, grammatical structure,")
    print("and emotional expression across literary periods from 1850-2014.")
    
    return results_df

if __name__ == '__main__':
    main()