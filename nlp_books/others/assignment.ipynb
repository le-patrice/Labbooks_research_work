{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c217bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d240b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a document and extract text and year\n",
    "def read_document(file_path):\n",
    "    # Extract year from filename if it exists (assuming year is 4 digits)\n",
    "    year_match = re.search(r'(\\d{4})', file_path)\n",
    "    year = year_match.group(1) if year_match else None\n",
    "    \n",
    "    if file_path.endswith('.pdf'): \n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    else:\n",
    "        raise ValueError(\"Non Existant file or Unsupported file format. Please provide a .pdf or .txt file.\")\n",
    "    \n",
    "    return text, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simple function that uses spacy to extract entities and returns a table from a document that consists the following colums: token, frequency, pos_tags,lemma, ner,stem, tense,label,suffix and year of the most common 100 tokens being appended to the dataframe\n",
    "# def extract_entities(text, year):\n",
    "#     doc = nlp(text)\n",
    "#     data = {\n",
    "#         \"token\": [],\n",
    "#         \"frequency\": [],\n",
    "#         \"pos_tags\": [],\n",
    "#         \"lemma\": [],\n",
    "#         \"ner\": [],\n",
    "#         \"stem\": [],\n",
    "#         \"tense\": [],\n",
    "#         \"label\": [],\n",
    "#         \"suffix\": [],\n",
    "#         \"year\": []\n",
    "#     }\n",
    "    \n",
    "#     for token in doc:\n",
    "#         if not token.is_stop and not token.is_punct:\n",
    "#             data[\"token\"].append(token.text)\n",
    "#             data[\"frequency\"].append(token.prob)\n",
    "#             data[\"pos_tags\"].append(token.pos_)\n",
    "#             data[\"lemma\"].append(token.lemma_)\n",
    "#             data[\"ner\"].append(token.ent_type_)\n",
    "#             data[\"stem\"].append(token._.stem)\n",
    "#             data[\"tense\"].append(token._.tense)\n",
    "#             data[\"label\"].append(token.dep_)\n",
    "#             data[\"suffix\"].append(token._.suffix)\n",
    "#             data[\"year\"].append(year)\n",
    "\n",
    "#     df = pd.DataFrame(data)\n",
    "#     df = df.groupby(\"token\").agg({\n",
    "#         \"frequency\": \"sum\",\n",
    "#         \"pos_tags\": \"first\",\n",
    "#         \"lemma\": \"first\",\n",
    "#         \"ner\": \"first\",\n",
    "#         \"stem\": \"first\",\n",
    "#         \"tense\": \"first\",\n",
    "#         \"label\": \"first\",\n",
    "#         \"suffix\": \"first\",\n",
    "#         \"year\": \"first\"\n",
    "#     }).reset_index()\n",
    "    \n",
    "#     df = df.sort_values(by=\"frequency\", ascending=False).head(100)\n",
    "    \n",
    "#     return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9beb76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function that uses spacy to extract entities and returns a table from a document that consists the following colums: token, frequency, pos_tags,lemma, ner,stem, tense,label,suffix and year of the docuement\n",
    "def extract_entities(text, year):\n",
    "    doc = nlp(text)\n",
    "    data = {\n",
    "        \"token\": [],\n",
    "        \"frequency\": [],\n",
    "        \"pos_tags\": [],\n",
    "        \"lemma\": [],\n",
    "        \"ner\": [],\n",
    "        \"stem\": [],\n",
    "        \"tense\": [],\n",
    "        \"label\": [],\n",
    "        \"suffix\": [],\n",
    "        \"year\": []\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        if  token.is_stop or  token.is_punct:\n",
    "            continue\n",
    "        data[\"token\"].append(token.text)\n",
    "        data[\"frequency\"].append(doc.count_by(token.i))\n",
    "        data[\"pos_tags\"].append(token.pos_)\n",
    "        data[\"lemma\"].append(token.lemma_)\n",
    "        data[\"ner\"].append(token.ent_type_)\n",
    "        data[\"stem\"].append(token._.stem if hasattr(token._, 'stem') else None)\n",
    "        data[\"tense\"].append(token.tag_)\n",
    "        data[\"label\"].append(token.dep_)\n",
    "        data[\"suffix\"].append(token.suffix_)\n",
    "        data[\"year\"].append(year)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "465ded9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499522"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scarlet, year = read_document('scarlet_letter_1850.txt')\n",
    "len(scarlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to visualize the frequency of tokens in a document\n",
    "def visualize_token_frequency(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=df, x='token', order=df['token'].value_counts().index)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Token Frequency')\n",
    "    plt.xlabel('Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7214305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function to show word usage over different documents which are written with in a span of years\n",
    "def visualize_word_usage_over_years(df, year_column='year', word_column='token'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=df, x=year_column, hue=word_column)\n",
    "    plt.title('Word Usage Over Years')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title=word_column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the distribution for parts of speech, named entities, frequency in a document\n",
    "def visualize_pos_and_ner_distribution(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # POS Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(data=df, x='pos_tags', order=df['pos_tags'].value_counts().index)\n",
    "    plt.title('Part of Speech Distribution')\n",
    "    plt.xlabel('POS Tags')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # NER Distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(data=df, x='ner', order=df['ner'].value_counts().index)\n",
    "    plt.title('Named Entity Recognition Distribution')\n",
    "    plt.xlabel('NER Tags')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A combined main function that takes in a list of file paths, reads the documents, extracts entities, and visualizes the results\n",
    "def main(file_paths):\n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        text, year = read_document(file_path)\n",
    "        df = extract_entities(text, year)\n",
    "        all_data.append(df)\n",
    "        \n",
    "        visualize_token_frequency(df)\n",
    "        visualize_pos_distribution(df)\n",
    "        visualize_ner_distribution(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d03e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assignment.ipynb             spacy.ipynb\n",
      "Martian_chronicles_1950.pdf  The_Vanishing_Half_2020.pdf\n",
      "scarlet_letter_1850.txt      tristram_shandy_1759.txt\n",
      "sister_carrie_1900.txt       White_Teeth_2000.pdf\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of the main function \n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = ['scarlet_letter_1850.txt']  # Replace with your file paths\n",
    "    combined_df = main(file_paths)\n",
    "    print(combined_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
